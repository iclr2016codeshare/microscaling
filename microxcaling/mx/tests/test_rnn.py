"""
Copyright (c) Microsoft Corporation.
Licensed under the MIT License.
"""

import pytest
import sys
from copy import deepcopy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

from .common_lib import check_diff

from mx.specs import finalize_mx_specs
from mx import LSTM

np.random.seed(0xdeadbeef)
torch.manual_seed(0xdeadbeef)

DEVICE__CUSTOM_CUDA = [
    ('cpu', False),
    ('cuda', False),
    ('cuda', True)
]


@pytest.mark.parametrize("C1, C2", [
    (torch.nn.LSTM, LSTM),
])
@pytest.mark.parametrize("Hin, Hc", [
    (50, 128),
])
@pytest.mark.parametrize("L, N, num_layers", [
    # L = seq len,
    # N = batch size
    (25, 8, 1),
    (25, 8, 3),
])
@pytest.mark.parametrize("bias", [False, True])
@pytest.mark.parametrize("bidir", [False, True])
@pytest.mark.parametrize("packed", [False, True])
@pytest.mark.parametrize("device, custom_cuda", DEVICE__CUSTOM_CUDA)
def test_rnn(C1, C2, Hin, Hc, L, N, num_layers, bias, bidir,
             packed, device, custom_cuda):
    # No-quantization mx specs
    mx_specs = {}
    mx_specs['bfloat'] = 32
    mx_specs['quantize_backprop'] = True
    mx_specs['custom_cuda'] = custom_cuda
    mx_specs = finalize_mx_specs(mx_specs, early_exit=False)

    # input is (L, N, Hin)
    m_ = np.ones((L, N, Hin))
    if packed:
        # Add pad tokens
        m_ = np.concatenate([m_, np.zeros_like(m_)], axis=0)

    m1 = torch.tensor(m_, dtype=torch.float32, device=device,
                      requires_grad=True)
    m2 = torch.tensor(m_, dtype=torch.float32, device=device,
                      requires_grad=True)

    c1 = C1(Hin, Hc, num_layers=num_layers, bias=bias,
            bidirectional=bidir)
    c2 = C2(Hin, Hc, num_layers=num_layers, bias=bias,
            bidirectional=bidir,
            mx_specs=mx_specs)

    with torch.no_grad():
        assert(c1._flat_weights_names == c2._flat_weights_names)
        for name in c1._flat_weights_names:
            c2.__setattr__(name, getattr(c1, name))
        c2.flatten_parameters()

    if device == 'cuda':
        c1 = c1.cuda()
        c2 = c2.cuda()

    if packed:
        p1 = pack_padded_sequence(m1, lengths=torch.tensor([L]*N))
        q1,h1 = c1(p1)
        q1, _ = pad_packed_sequence(q1)
    else:
        q1,h1 = c1(m1)
    loss1 = (q1**2).mean().sqrt()
    loss1.backward()
    torch.cuda.synchronize()
    mem_alloc1 = torch.cuda.max_memory_allocated() / 1024**2
    mem_res1 = torch.cuda.max_memory_reserved() / 1024**2
    torch.cuda.reset_peak_memory_stats()
    
    if packed:
        p2 = pack_padded_sequence(m2, lengths=torch.tensor([L]*N))
        q2,h2 = c2(p2)
        q2, _ = pad_packed_sequence(q2)
    else:
        q2,h2 = c2(m2)
    loss2 = (q2**2).mean().sqrt()
    loss2.backward()
    torch.cuda.synchronize()
    mem_alloc2 = torch.cuda.max_memory_allocated() / 1024**2
    mem_res2 = torch.cuda.max_memory_reserved() / 1024**2
    torch.cuda.reset_peak_memory_stats()

    check_diff(q1, q2, tol=1e-6)
    check_diff(h1[0], h2[0], tol=1e-6)
    check_diff(h1[1], h2[1], tol=1e-6)
    check_diff(m1.grad, m2.grad, tol=1e-6)
    for name in c1._flat_weights_names:
        check_diff(getattr(c1,name).grad,
                   getattr(c2,name).grad,
                   tol=1e-6)

    # Keep mem overhead to 3x
    assert(mem_alloc2 <= 3*mem_alloc1)
    assert(mem_res2 <= 3*mem_res1)


@pytest.mark.parametrize("C1, C2", [
    (torch.nn.LSTM, LSTM),
])
@pytest.mark.parametrize("Hin, Hc", [
    (50, 128),
])
@pytest.mark.parametrize("L, N", [
    (25, 8),
])
@pytest.mark.parametrize("dropout", [0.5, 0.9])
@pytest.mark.parametrize("device, custom_cuda", DEVICE__CUSTOM_CUDA)
def test_rnn_dropout(C1, C2, Hin, Hc, L, N, dropout,
                     device, custom_cuda):
    # No-quantization mx specs
    mx_specs = {}
    mx_specs['bfloat'] = 32
    mx_specs['quantize_backprop'] = True
    mx_specs['custom_cuda'] = custom_cuda
    mx_specs = finalize_mx_specs(mx_specs, early_exit=False)

    num_layers = 2  # dropout occurs between hidden layers
    bias = False
    bidir = True

    # input is (L, N, Hin)
    # For dropout, set all inputs to 1.0
    m_ = np.ones((L, N, Hin))
    m1 = torch.tensor(m_, dtype=torch.float32, device=device,
                      requires_grad=True)
    m2 = torch.tensor(m_, dtype=torch.float32, device=device,
                      requires_grad=True)

    c1 = C1(Hin, Hc, num_layers=num_layers, bias=bias,
            dropout=dropout, bidirectional=bidir)
    c2 = C2(Hin, Hc, num_layers=num_layers, bias=bias,
            dropout=dropout, bidirectional=bidir,
            mx_specs=mx_specs)

    with torch.no_grad():
        assert(c1._flat_weights_names == c2._flat_weights_names)
        for name in c1._flat_weights_names:
            # For dropout, set all weights to 1.0
            if dropout > 0:
                x = getattr(c1, name)
                v = torch.nn.parameter.Parameter(
                        torch.ones(x.shape, dtype=x.dtype,
                                   device=device))
                c1.__setattr__(name, v)
            c2.__setattr__(name, getattr(c1, name))
        c2.flatten_parameters()

    if device == 'cuda':
        c1 = c1.cuda()
        c2 = c2.cuda()

    # For dropout we can only diff mean square
    def msq(x):
        return (x**2).mean().sqrt()

    q1,h1 = c1(m1)
    loss1 = (q1**2).mean().sqrt()
    loss1.backward()
    torch.cuda.synchronize()

    q2,h2 = c2(m2)
    loss2 = (q2**2).mean().sqrt()
    loss2.backward()
    torch.cuda.synchronize()

    check_diff(loss1, loss2, tol=1e-6)
    check_diff(msq(h1[0]), msq(h2[0]), tol=1e-6)
    check_diff(msq(h1[1]), msq(h2[1]), tol=1e-6)
    check_diff(msq(m1.grad), msq(m2.grad), tol=1e-6)
    for name in c1._flat_weights_names:
        check_diff(msq(getattr(c1,name).grad),
                   msq(getattr(c2,name).grad),
                   tol=1e-6)

